---
title: "IntroEmpiricalBayes"
author: "Jeff Grayum"
date: "6/14/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(ggthemes)
library(Lahman)
library(stats4)
#install.packages("gamlss")
library(gamlss)
library(broom)
library(splines)
```


Ch. 2.3 Conjugate prior
```{r}
#Number of Bernouli trials.
num_trials <- 10e6

#From book -> alpha = 81, beta = 29.  Data frame has 1 col of true average, and one of hits (out of 300).
simulations <- data_frame(
  true_average = 
    rbeta(num_trials, 81, 219),
  hits = rbinom(num_trials, 300,
                true_average)
)

simulations 

#How many got 100/300 (as in our hypothetical player?).

hits_100 <- simulations %>%
  filter(hits == 100) %>%
  view()

hits_100 %>%
  ggplot(aes(true_average)) +
  geom_histogram(bins = 50) +
  theme_fivethirtyeight() +
  labs(title = "Distribution of true batting averages for players with 100/300 hits",
       x = "True batting average",
       y = "Count") +
  theme(axis.title = element_text(),
        text = element_text(family = "Times New Roman")) +
  geom_vline(aes(xintercept = 0.3), color = "grey", linetype = 2)

#What if th eplayer had gotten 60, or 80 hits, out of 300?  Let's plot the density of each of these subsets of the simulation.

simulations %>%
  filter(hits %in% c(60, 80, 100)) %>%
  ggplot(aes(true_average, color = factor(hits))) +
  geom_density() +
  labs(x = "True average of players with H hits / 300 at-bats",
       color = "H",
       y = "Density",
       title = "Number of H hits / 300 and distribution of true batting averages") +
  theme_fivethirtyeight() +
  theme(axis.title = element_text(),
        text = element_text(family = "Times New Roman"))
```

Now, we will begin looking at the batting dataset from the Lahman package.
First, some cleaning.
```{r}
#Filter out those pitchers.
Batting %>%
  view()

#Instead of joining batting and pitching datasets, we are filtering out players in the batting dataset who are in the pitching dataset (anit_join), by matching the playerID.  Also filtering out any players with 0 at-bats.  Then we are summarizing the total hits, total at bats, and creating a new column of batting averages (H / AB) for each player (group_by).
career <- Batting %>%
  filter(AB > 0) %>%
  anti_join(Pitching, by = "playerID") %>%
  group_by(playerID) %>%
  summarize(H = sum(H),
            AB = sum(AB)) %>%
  mutate(average = H / AB)

#Now, we'll include names with the playerID's.
career <- Master %>%
  tbl_df() %>%
  dplyr::select(playerID, nameFirst, nameLast) %>%
  unite(name, nameFirst, nameLast, sep = " ") %>%
  inner_join(career, by = "playerID") 

#Notice, if we arrange by average, all the top players have very few at-bats!
career %>%
  arrange(desc(average))

#The worst batters also have very few at-bats!
career %>%
  arrange(average)

#Out of curiosity, who has a 0 average with the most at bats?
career %>%
  filter(average == 0) %>%
  arrange(desc(AB))

#These are both pretty bad ways to estimate who is the best and who is the worst!
```

3.2 Step 1.  Estimate a prior from all your data!
```{r}
career %>%
  filter(AB >= 500) %>%
  ggplot(aes(average)) +
  geom_histogram(bins = 30)

career_filtered <- career %>%
  filter(AB > 500) 

#log-likelihood function
ll <- function(alpha, beta) {
  x <- career_filtered$H
  total <- career_filtered$AB
  -sum(VGAM::dbetabinom.ab(x, total, alpha, beta, log = TRUE))
}

#Maximum likelihood estimation
m <- mle(ll, start = list(alpha = 1, beta = 10), method = "L-BFGS-B",
         lower = c(0.0001, .1))

ab <- coef(m)
alpha0 <- ab[1]  
beta0 <- ab[2]

#My estimates differ slightly from book... maybe dataset has been updated?
```

Let's make a n empirical bayes estimate for all the batters (update our conjugate prior)
```{r}

career_eb <- career %>%
  mutate(eb_estimate = (H + alpha0) / (AB + alpha0 + beta0)) %>%
  arrange(desc(eb_estimate))

#Looking at the top 50 estimated batting averages.
career_eb %>%
  arrange(desc(eb_estimate)) %>%
  head(50)



#looking at the worst 10
career_eb %>%
  arrange(desc(eb_estimate)) %>%
  tail(10) %>%
  view()

#Notice, all these players have a fair amount of at-bats!

career_eb %>%
  ggplot(aes(average, eb_estimate, color = AB)) +
  geom_point() +
  theme_few() +
  geom_hline(aes(yintercept = 0.261), linetype = 2, color = "red") +
  labs(x = "Batting average (hits / at-bats)",
       y = "Empirical Bayes batting average",
       title = "Realized batting average vs EB estimated batting average",
       color = "At-bats") 
```

Adding credible intervals to our dataset.
```{r}
#Below values estimated in previous chunk (prior parameters for each distribution).
alpha0 <- 101.4
beta0 <- 287.3

#Another term for our new eb_estimate (an updated batting average), is Point Estimate.  Generally, these eb estimates are pushed towards the average (shrinkage). This value is much more useful than our raw estimate, as it filters out the noise generated by our low counts.
career_eb <- career %>%
  mutate(eb_estimate = (H + alpha0) / (AB + alpha0 + beta0)) %>%
  arrange(desc(eb_estimate)) %>%
  view()

#We still have uncertainty within our eb estimate, and the degree of uncertainty varies between players (fewer at-bats == more uncertainty)! We want a integral of possible batting averages for each player.

#First, let's calculate the posterior shape parameters for each player, and add them to our dataset.

career_eb <- career_eb %>%
  mutate(alpha1 = alpha0 + H,
         beta1 = beta0 + AB - H) %>%
  arrange(desc(eb_estimate)) %>%
  view()

#We can now visualize the density of the posterior distribution for each. Let's only select a few:
career_eb %>%
  filter(name %in% c("Bernie Williams", "Chuck Knowlauch", "Darryl Strawberry", "Derek Jeter", "Jorge Posada", "Scott Brosius", "Tino Martinez")) 
#We actually can't because Im confused.Will come back to with more info later.
  
#We can however compute the edges of our confidence interval for each, using qbeta (quanitle of beta).
career_eb %>%
  filter(name == "Bernie Williams") %>%
  view()


yankee_1998_career <- career_eb %>%
  filter(name %in% c("Scott Brosius", "Derek Jeter", "Chuck Knoblauch", "Tino Martinez", "Jorge Posada", "Darryl Strawberry", "Bernie Williams"),
         AB > 173) %>%
  view()

yankee_1998_career <- yankee_1998_career %>%
  mutate(low = qbeta(0.25, alpha1, beta1),
         high = qbeta(0.975, alpha1, beta1)) %>%
  view()
  
  
yankee_1998_career %>%
  mutate(name = fct_reorder(name, eb_estimate)) %>%
  ggplot(aes(eb_estimate, name)) +
  geom_point() +
  geom_errorbarh(aes(xmin = low, xmax = high)) +
  geom_vline(xintercept = alpha0 / (alpha0 + beta0), color = "red", lty = 2) +
  labs(x = "Estimated batting average (w/ 95% interval)",
       y = "Player",
       title = "Empirical bayes estimated batting averages for 1998 Yankees",
       caption = "Vline = historical batting average") +
  theme_tufte()
```

Ch. 5 Hypothesis testing and FDR
```{r}
career_eb %>%
  filter(name == "Hank Aaron") %>%
  print()

#pbeta(testingHypo, alpha1, beta1)
pbeta(0.3, 3873, 8883)
#Thus, the probability of Hank Aaron's TRUE batting average of being less than .300 is 18.7%
#This is called THE POSTERIOR ERROR PROBABILITY


#Let's add the posterior error probability (that average is below .300) for each player!
career_eb <- career_eb %>%
  mutate(PEP = pbeta(0.300, alpha1, beta1)) %>%
  arrange(PEP) %>%
  view()

career_eb %>%
  ggplot(aes(PEP)) +
  geom_histogram() +
  theme_tufte() +
  labs(x = "Posterior error probility",
       y = "Count")


career_eb %>%
  ggplot(aes(eb_estimate, PEP, color = AB)) +
  geom_point(alpha = 0.5) +
  theme_few() +
  labs(x = "Empirical bayes estimated batting average",
       y = "Posterior error probability",
       color = "At-bats",
       caption = "As EB batting ave. increases, the PEP of ave. being below .300 decreases",
       title = "Shrunken batting ave. vs PEP of ave. > 0.300") +
  geom_vline(aes(xintercept = 0.3), lty = 2, color = "red")
```

Ch.5.3
```{r}
top_100 <- career_eb %>%
  arrange(PEP) %>% 
  head(100) %>%
  view()

top_200 <- career_eb %>%
  arrange(PEP) %>%
  head(200)

top_100 %>%
  summarise(total_errors = sum(PEP))
#Thus, 6.25 players in our top 100 shouldnt have been included in our H.O.F. (ave. is actually less than 0.300)
#(6.25/100)

sorted_PEP <- career_eb %>%
  arrange(PEP) %>% 
  view()

sum(top_100$PEP)
mean(top_100$PEP)

two_hundred_ave <- (sum(top_200$PEP)) / 200

mean(top_200$PEP)

mean(head(sorted_PEP$PEP, 50))

mean(head(sorted_PEP$PEP, 200))


#Lets just write a function.
mean_PEP <- function(p) {
  ave_PEP <- mean(head(sorted_PEP$PEP, p))
  return(ave_PEP)
}

mean_PEP(50) 

mean_PEP(100)

mean_PEP(200)

mean_PEP(100000)

#Let's write a loop, because i forgot how.
for(i in 1:length(top_100)) {
  print(paste(top_100$name[i], "is ranked number", i, "in the all time greatest batters list, with an shrunken average of", top_100$eb_estimate[i]))
}
```

Ch. 5.4 Q-values.
```{r}
career_eb <- career_eb %>%
  arrange(PEP) %>%
  mutate(qvalue = cummean(PEP)) %>%
  view()

#Now, if we want less than 5% error, only include players with a q-value of <.05
career_eb %>%
  filter(qvalue <= 0.05) %>%
  view()
```

Bayesian A/B testing
```{r}
#Grab career batting average of non-pitchers (pitched <= 3 games) %>%
pitchers <- Pitching %>%
  group_by(playerID) %>%
  summarize(gamesPitched = sum(G)) %>%
  filter(gamesPitched > 3)

career <- Batting %>%
  filter(AB > 0) %>%
  anti_join(pitchers, by = "playerID") %>%
  summarize(H = sum(H),
            AB = sum(AB)) %>%
  mutate(average = H / AB)  


career <- Master %>%
  tbl_df() %>%
  dplyr::select(playerID, nameFirst, nameLast, bats) %>%
  unite(name, nameFirst, nameLast, sep = " " ) %>%
  inner_join(career, by = "playerID")

alpha0 <- 101.4
beta0 <- 287.3



#Let's compare Hank Aaron adn Mike Piazza
aaron <- career_eb %>%
  filter(name == "Hank Aaron")

piazza <- career_eb %>%
  filter(name == "Mike Piazza")

two_players <- bind_rows(aaron, piazza) %>%
  view()
```

6.2.1 Simulation of posterior draws.
```{r}
#Draw a million items from each players distribution using rbeta, compare the results.

piazza_simulation <- rbeta(1000000, piazza$alpha1, piazza$beta1)
aaron_simulation <- rbeta(1000000, aaron$alpha1, aaron$beta1)

sim_results <- mean(piazza_simulation > aaron_simulation)

sim_results
#This gives us a 59% chance that Piazza is better than Aaron
```

6.2.2 Integration
```{r}
#Understand the section in the book and the concept, unsure why 0.29, 0.33 were chosen...
d <- 0.00002
limits <- seq(0.29, 0.33, d)
sum(outer(limits, limits, function(x,y) {
  (x > y) *
    dbeta(x, piazza$alpha1, piazza$beta1) *
    dbeta(y, aaron$alpha1, aaron$beta1) *
    d ^ 2
}))
```

6.2.3 CLosed-form solution
```{r}
#Prob that a random draw from A is greater than a random draw from B.  Uses calculus n stuff.

h <- function(alpha_a, beta_a, 
              alpha_b, beta_b) {
 
   j <- seq.int(0, round(alpha_b)
              -1)
 
   log_vals <- (lbeta(alpha_a + j, beta_a + beta_b) - log(beta_b + j) -
                 lbeta(1 + j, beta_b) - lbeta(alpha_a, beta_a))
  
   1-sum(exp(log_vals))
   
}

h(piazza$alpha1, piazza$beta1,
  aaron$alpha1, aaron$beta1)

#Not a fucking clue why this works.
```

6.2.4 Closed-form approximation
```{r}
#as alpha1 and beta1 become, they very much look like a normal distribution.  Much easier to calculate the probability that one normal distribution is greater than the other, than beta with beta distributions!

h_approx <- function(alpha_a, beta_a, alpha_b, beta_b){
  
  u1 <- alpha_a / (alpha_a + beta_a)
  
  u2 <- alpha_b / (alpha_b + beta_b)
  
  var1 <- (alpha_a * beta_a) /
    ((alpha_a + beta_a) ^ 2 *
       (alpha_a + beta_a + 1))
  
  var2 <- (alpha_b * beta_b) /
    ((alpha_b + beta_b) ^ 2 *
     (alpha_b + beta_b + 1))
    
  pnorm(0, u2 - u1, sqrt(var1 + var2))
}

h_approx(piazza$alpha1, piazza$beta1, aaron$alpha1, aaron$beta1)
```

Chi-squared test to determine if there is a statistical difference between two players averages.
```{r}
prop.test(two_players$H, two_players$AB)

#P-value of 0.7, the test couldn't find a difference.
```

6.3 Confidence and credible intervals
```{r}
credible_interval_approx <- function (a, b, c, d) {
  u1 <- a / (a + b)
  u2 <- c / (c + d) 
  var1 <- a * b / ((a + b) ^ 2 * (a + b + 1))
  var2 <- c * d / ((c + d) ^ 2 * (c + d + 1))
  
  mu_diff <- u2 - u1
  sd_diff <- sqrt(var1 + var2)
  
  data_frame(posterior = pnorm(0, mu_diff, sd_diff),
             estimate = mu_diff, 
             conf.low = qnorm(0.025, mu_diff, sd_diff),
             conf.high = qnorm(0.975, mu_diff, sd_diff))
}

credible_interval_approx(piazza$alpha1, piazza$beta1,
                         aaron$alpha1, aaron$beta1)


random_20 <- sample_n(career_eb, 20) %>%
  select(-playerID) %>%
  view()
```

7.1
```{r}

#Notice that the better players are (logically) getting more AB's!
orig_ave <- career_eb %>%
  filter((H / AB) < 0.5) %>%
  ggplot(aes(AB, (H / AB))) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE, color = "gray") +
  theme_tufte() +
  scale_x_log10() +
  scale_y_continuous() +
  labs(title = "Number of at-bats compared to orginal and adjusted batting averages",
       x = "At-bats (log scale)",
       y = "Raw batting ave. (H/AB)") +
  expand_limits(y = c(0, 0.5)) +
  geom_hline(aes(yintercept = 0.261), color = "orange", lty = 2)

shrunken_eb <- career_eb %>%
  ggplot(aes(AB, eb_estimate)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE, color = "grey") +
  theme_tufte() +
  scale_x_log10() +
  scale_y_continuous() +
  labs(x = "At-bats (log scale)",
       y = "Empirical Bayes batting average") +
  expand_limits(y = c(0, 0.5)) +
  geom_hline(aes(yintercept = 0.261), color = "orange", lty = 2)

few %>%
  summarize(median_ave = median(average))
```

We will account for the fact that the better players get more at-bats by fitting beta-binomial regression using maximum likelihood.
```{r}
fit <- gamlss(cbind(H, AB - H) ~ log(AB),
              data = career_eb,
              family = BB(mu.link = "identity"))

#Now, let's pull out the coefficients using tidy(), from the broom package.
td <- tidy(fit)
td
#This means that our new prior beta distribution *depends* on the value of AB.

#First, calculate the mu and sigma parameters for each person.
mu <- fitted(fit, parameter = "mu")
sigma <- fitted(fit, parameter = "sigma")

head(mu)
head(sigma)

#Now, we can calculate alpha0 and beta0 parameters for each person (our priors), and then update with each players H and AB's.

career_eb <- career_eb %>%
  dplyr::select(name, H, AB, original_eb = eb_estimate) %>%
  mutate(mu = mu,
         alpha0 = mu / sigma,
         beta0 = (1 - mu) / sigma,
         alpha1 = alpha0 + H,
         beta1 = beta0 + AB - H,
         estimate = alpha1 / (alpha1 + beta1)) %>%
  view()

career_eb_wAB %>%
  ggplot(aes(original_eb, new_eb, color = AB)) +
  geom_point(alpha = 0.7) +
  theme_tufte() +
  labs(x = "Original EB estimate",
       y = "EB estimate with AB term",
       color = "At-bats",
       title = "How does including the AB term affect EB estimates?")



new_eb <- career_eb_wAB %>%
  ggplot(aes(AB, new_eb)) +
  geom_point()  +
  scale_x_log10() +
  geom_smooth(method = lm, color = "gray") +
  theme_tufte() +
  labs(x = "At-bats (log scale)",
       y = "EB estimate (including AB term)",
       caption = "Notice: we now shrink towards *trend fit*, not overall ave.") +
 expand_limits(y = c(0, 0.5)) +
  geom_hline(aes(yintercept = 0.261), color = "orange", lty = 2)


career_eb_wAB %>%
  view()

compare_aves <- gridExtra::grid.arrange(orig_ave, shrunken_eb, new_eb, ncol = 3)

compare_aves
```

Ch 8. Empirical Bayes Hierarchical Modeling
```{r}
career_eb <- career_eb %>%
  dplyr::select(name, H, AB, original_eb = eb_estimate) %>%
  mutate(mu = mu,
         alpha0 = mu / sigma,
         beta0 = (1 - mu) / sigma,
         alpha1 = alpha0 + H,
         beta1 = beta0 + AB - H,
         estimate = alpha1 / (alpha1 + beta1)) %>%
  view()
```

8.2 Right and left-handed batters
```{r}
#Now we will incorporate "bats" to the formula in the formula in the gamlss call (our beta binomial regression)
pitchers <- Pitching %>%
  group_by(playerID) %>%
  summarize(gamesPitched = sum(G)) %>%
  filter(gamesPitched > 3)

career <- Batting %>%
  filter(AB > 0) %>%
  anti_join(pitchers, by = "playerID") %>%
  group_by(playerID) %>%
  summarise(H = sum(H), AB = sum(AB), year = mean(yearID)) %>%
  mutate(average = H/AB)

career <- Master %>%
  tbl_df() %>%
  dplyr::select(playerID, nameFirst, nameLast, bats) %>%
  unite(name, nameFirst, nameLast, sep = " ") %>%
  inner_join(career, by = "playerID") 

fit <- gamlss(cbind(H, AB - H) ~ log(AB),
              data = dplyr::select(career, -bats),
              family = BB(mu.link = "identity"))

career_eb <- career %>%
  mutate(mu = fitted(fit, "mu"),
         sigma = fitted(fit, "sigma"),
         alpha0 = mu / sigma,
         beta0 = (1 - mu) / sigma,
         alpha1 = alpha0 + H,
         beta1 = beta0 + AB - H,
         estimate = alpha1 / (alpha1 + beta1))

career2 <- career %>%
  filter(!is.na(bats)) %>%
  mutate(bats = relevel(bats, "R")) 

fit2 <- gamlss(cbind(H, AB - H)
               ~log(AB) + bats,
               data = career2,
               family = BB(mu.link = "identity"))

tidy(fit2)
```

8.3 Over time
```{r}
#Notice how averages change over time! We need to incorporate this nonlinear trend into our priors.
career2 %>%
  mutate(decade = (year %/%10) * 10) %>%
  filter(AB >= 500) %>%
  ggplot(aes(decade, average, group = decade)) +
  geom_boxplot() +
  theme_few() +
  labs(title = "Mean batting averages across decades",
       x = "Decade",
       y = "Mean batting average") 

#We will incorporate the trend using a natural cubic spine, using the ns function.
fit3 <- gamlss(cbind(H, AB - H)
               ~0 + ns(year, df = 5) +
                 bats + log(AB),
               data = career2,
               family = BB(mu.link = "identity"))
```

8.3.1
```{r}
#The advantage of being left handed changes over time...let's change the formula to allow for an interaction term that allows the advantage of being left handed change over time!

fit4 <- gamlss(cbind(H, AB - H)
               ~0 + ns(year, 5) * bats +
                 log(AB),
               data = career2,
               family = BB(mu.link = "identity")) 

```

