---
title: "IntroEmpiricalBayes"
author: "Jeff Grayum"
date: "6/14/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(ggthemes)
library(Lahman)
library(stats4)
```


Ch. 2.3 Conjugate prior
```{r}
#Number of Bernouli trials.
num_trials <- 10e6

#From book -> alpha = 81, beta = 29.  Data frame has 1 col of true average, and one of hits (out of 300).
simulations <- data_frame(
  true_average = 
    rbeta(num_trials, 81, 219),
  hits = rbinom(num_trials, 300,
                true_average)
)

simulations 

#How many got 100/300 (as in our hypothetical player?).

hits_100 <- simulations %>%
  filter(hits == 100) %>%
  view()

hits_100 %>%
  ggplot(aes(true_average)) +
  geom_histogram(bins = 50) +
  theme_fivethirtyeight() +
  labs(title = "Distribution of true batting averages for players with 100/300 hits",
       x = "True batting average",
       y = "Count") +
  theme(axis.title = element_text(),
        text = element_text(family = "Times New Roman")) +
  geom_vline(aes(xintercept = 0.3), color = "grey", linetype = 2)

#What if th eplayer had gotten 60, or 80 hits, out of 300?  Let's plot the density of each of these subsets of the simulation.

simulations %>%
  filter(hits %in% c(60, 80, 100)) %>%
  ggplot(aes(true_average, color = factor(hits))) +
  geom_density() +
  labs(x = "True average of players with H hits / 300 at-bats",
       color = "H",
       y = "Density",
       title = "Number of H hits / 300 and distribution of true batting averages") +
  theme_fivethirtyeight() +
  theme(axis.title = element_text(),
        text = element_text(family = "Times New Roman"))
```

Now, we will begin looking at the batting dataset from the Lahman package.
First, some cleaning.
```{r}
#Filter out those pitchers.
Batting %>%
  view()

#Instead of joining batting and pitching datasets, we are filtering out players in the batting dataset who are in the pitching dataset (anit_join), by matching the playerID.  Also filtering out any players with 0 at-bats.  Then we are summarizing the total hits, total at bats, and creating a new column of batting averages (H / AB) for each player (group_by).
career <- Batting %>%
  filter(AB > 0) %>%
  anti_join(Pitching, by = "playerID") %>%
  group_by(playerID) %>%
  summarize(H = sum(H),
            AB = sum(AB)) %>%
  mutate(average = H / AB)

#Now, we'll include names with the playerID's.
career <- Master %>%
  tbl_df() %>%
  dplyr::select(playerID, nameFirst, nameLast) %>%
  unite(name, nameFirst, nameLast, sep = " ") %>%
  inner_join(career, by = "playerID") %>%
  dplyr::select(-playerID)

#Notice, if we arrange by average, all the top players have very few at-bats!
career %>%
  arrange(desc(average))

#The worst batters also have very few at-bats!
career %>%
  arrange(average)

#Out of curiosity, who has a 0 average with the most at bats?
career %>%
  filter(average == 0) %>%
  arrange(desc(AB))

#These are both pretty bad ways to estimate who is the best and who is the worst!
```

3.2 Step 1.  Estimate a prior from all your data!
```{r}
career %>%
  filter(AB >= 500) %>%
  ggplot(aes(average)) +
  geom_histogram(bins = 30)

career_filtered <- career %>%
  filter(AB > 500) 

#log-likelihood function
ll <- function(alpha, beta) {
  x <- career_filtered$H
  total <- career_filtered$AB
  -sum(VGAM::dbetabinom.ab(x, total, alpha, beta, log = TRUE))
}

#Maximum likelihood estimation
m <- mle(ll, start = list(alpha = 1, beta = 10), method = "L-BFGS-B",
         lower = c(0.0001, .1))

ab <- coef(m)
alpha0 <- ab[1]  
beta0 <- ab[2]

#My estimates differ slightly from book... maybe dataset has been updated?
```

